#!/usr/bin/env python3
"""
ğŸ“Š ADVANCED STATISTICS ENGINE
Sistema de anÃ¡lise estatÃ­stica avanÃ§ada para dados de tÃªnis de mesa.
CorrelaÃ§Ãµes, regressÃµes, anÃ¡lise preditiva e insights estatÃ­sticos profundos.
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import logging
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import warnings

# Imports do sistema
from progression_analyzer import ProgressionAnalyzer
from fast_comparison_engine import FastComparisonEngine
from benchmark_optimizer import BenchmarkOptimizer

warnings.filterwarnings('ignore')

@dataclass
class StatisticalInsight:
    """Insight estatÃ­stico individual"""
    insight_type: str  # 'correlation', 'trend', 'anomaly', 'prediction'
    title: str
    description: str
    statistical_significance: float  # p-value or confidence
    effect_size: float  # magnitude of effect
    data_points: int
    visualization_data: Optional[Dict[str, Any]]
    actionable_recommendation: str
    confidence_level: str  # 'high', 'medium', 'low'

@dataclass
class UserClusterProfile:
    """Perfil de cluster de usuÃ¡rios"""
    cluster_id: int
    cluster_name: str
    typical_characteristics: Dict[str, float]
    user_count: int
    improvement_pattern: str
    recommended_approach: str
    success_factors: List[str]

class AdvancedStatisticsEngine:
    """
    ğŸ“Š Motor de EstatÃ­sticas AvanÃ§adas
    AnÃ¡lise estatÃ­stica profunda de dados de performance e progressÃ£o
    """
    
    def __init__(self, data_directory: str = "./"):
        self.data_dir = Path(data_directory)
        
        # Componentes do sistema
        self.progression_analyzer = ProgressionAnalyzer()
        self.fast_engine = FastComparisonEngine()
        
        # Cache de dados
        self._data_cache = {}
        self._analysis_cache = {}
        
        # ConfiguraÃ§Ã£o de visualizaÃ§Ã£o
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        print("ğŸ“Š Advanced Statistics Engine inicializado")
    
    def collect_all_user_data(self) -> pd.DataFrame:
        """ğŸ“ˆ Coleta todos os dados de usuÃ¡rios disponÃ­veis"""
        self.logger.info("ğŸ“ˆ Coletando dados de todos os usuÃ¡rios...")
        
        all_data = []
        
        # Buscar arquivos de progressÃ£o
        progression_files = list(self.data_dir.glob("progression_database/*_progression.json"))
        
        for prog_file in progression_files:
            try:
                with open(prog_file, 'r', encoding='utf-8') as f:
                    user_data = json.load(f)
                
                user_id = user_data['user_id']
                
                for entry in user_data.get('entries', []):
                    # Extrair dados estruturados
                    data_point = {
                        'user_id': user_id,
                        'timestamp': entry['timestamp'],
                        'score': entry['score'],
                        'confidence': entry['confidence'],
                        'cycles_detected': entry['cycles_detected'],
                        'movement': entry['movement'],
                        'professional_reference': entry['professional_reference'],
                        'preparation_score': entry.get('phase_scores', {}).get('preparation', 0),
                        'contact_score': entry.get('phase_scores', {}).get('contact', 0),
                        'follow_through_score': entry.get('phase_scores', {}).get('follow_through', 0),
                        'overall_quality_score': entry.get('phase_scores', {}).get('overall_quality', 0),
                    }
                    
                    # Adicionar mÃ©tricas tÃ©cnicas se disponÃ­veis
                    if 'technical_metrics' in entry:
                        tech_metrics = entry['technical_metrics']
                        data_point.update({
                            'phase_consistency': tech_metrics.get('phase_consistency', 0),
                            'cycle_ratio': tech_metrics.get('cycle_ratio', 0),
                            'confidence_numeric': tech_metrics.get('confidence_numeric', 0)
                        })
                    
                    all_data.append(data_point)
                    
            except Exception as e:
                self.logger.warning(f"Erro ao processar {prog_file}: {e}")
        
        if not all_data:
            # Gerar dados simulados para demonstraÃ§Ã£o
            all_data = self._generate_demo_data()
        
        df = pd.DataFrame(all_data)
        
        # Processamento adicional
        if not df.empty:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['days_since_start'] = (df['timestamp'] - df['timestamp'].min()).dt.days
            df['session_number'] = df.groupby('user_id').cumcount() + 1
            
            # Calcular mÃ©tricas derivadas
            df['improvement_rate'] = df.groupby('user_id')['score'].transform(
                lambda x: x.diff().fillna(0)
            )
            
            df['cumulative_improvement'] = df.groupby('user_id')['improvement_rate'].cumsum()
            
        self.logger.info(f"ğŸ“Š Coletados {len(df)} pontos de dados de {df['user_id'].nunique() if not df.empty else 0} usuÃ¡rios")
        
        return df
    
    def _generate_demo_data(self) -> List[Dict]:
        """ğŸ² Gera dados simulados para demonstraÃ§Ã£o"""
        demo_data = []
        users = ['Americo', 'Baixinha', 'Gordo', 'Usuario1', 'Usuario2']
        professionals = ['Ma_Long', 'Fan_Zhendong', 'Zhang_Jike', 'Timo_Boll']
        movements = ['FD', 'BD', 'FP']
        
        base_date = datetime.now() - timedelta(days=90)
        
        for user in users:
            sessions = np.random.randint(5, 25)  # 5-25 sessÃµes por usuÃ¡rio
            base_score = np.random.uniform(45, 75)  # Score inicial
            improvement_rate = np.random.uniform(-0.2, 1.5)  # Taxa de melhoria
            
            for session in range(sessions):
                date = base_date + timedelta(days=session * np.random.randint(1, 5))
                
                # Score com tendÃªncia e ruÃ­do
                score = base_score + (session * improvement_rate) + np.random.normal(0, 3)
                score = np.clip(score, 20, 95)  # Limitar score
                
                # Gerar scores por fase
                phase_variance = np.random.uniform(0.8, 1.2, 4)
                phases = {
                    'preparation': score * phase_variance[0],
                    'contact': score * phase_variance[1], 
                    'follow_through': score * phase_variance[2],
                    'overall_quality': score * phase_variance[3]
                }
                
                demo_data.append({
                    'user_id': user,
                    'timestamp': date.isoformat(),
                    'score': score,
                    'confidence': np.random.choice(['high', 'medium', 'low'], p=[0.5, 0.3, 0.2]),
                    'cycles_detected': np.random.randint(2, 8),
                    'movement': np.random.choice(movements),
                    'professional_reference': np.random.choice(professionals),
                    'preparation_score': phases['preparation'],
                    'contact_score': phases['contact'],
                    'follow_through_score': phases['follow_through'],
                    'overall_quality_score': phases['overall_quality'],
                    'phase_consistency': np.random.uniform(0.6, 0.9),
                    'cycle_ratio': np.random.uniform(0.3, 1.2),
                    'confidence_numeric': {'high': 1.0, 'medium': 0.6, 'low': 0.3}[
                        np.random.choice(['high', 'medium', 'low'], p=[0.5, 0.3, 0.2])
                    ]
                })
        
        return demo_data
    
    def analyze_correlation_patterns(self, df: pd.DataFrame) -> List[StatisticalInsight]:
        """ğŸ“ˆ Analisa padrÃµes de correlaÃ§Ã£o entre variÃ¡veis"""
        self.logger.info("ğŸ“ˆ Analisando padrÃµes de correlaÃ§Ã£o...")
        
        insights = []
        
        # Selecionar variÃ¡veis numÃ©ricas
        numeric_cols = ['score', 'cycles_detected', 'preparation_score', 'contact_score',
                       'follow_through_score', 'overall_quality_score', 'phase_consistency',
                       'cycle_ratio', 'confidence_numeric', 'session_number', 'improvement_rate']
        
        available_cols = [col for col in numeric_cols if col in df.columns]
        
        if len(available_cols) < 2:
            return insights
        
        # Calcular matriz de correlaÃ§Ã£o
        corr_matrix = df[available_cols].corr()
        
        # Encontrar correlaÃ§Ãµes significativas
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > 0.5:  # CorrelaÃ§Ã£o moderada a forte
                    var1 = corr_matrix.columns[i]
                    var2 = corr_matrix.columns[j]
                    strong_correlations.append((var1, var2, corr_value))
        
        # Gerar insights para correlaÃ§Ãµes fortes
        for var1, var2, corr_value in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True)[:5]:
            correlation_type = "positiva" if corr_value > 0 else "negativa"
            strength = "forte" if abs(corr_value) > 0.7 else "moderada"
            
            insight = StatisticalInsight(
                insight_type='correlation',
                title=f"CorrelaÃ§Ã£o {strength} {correlation_type}",
                description=f"{var1} e {var2} mostram correlaÃ§Ã£o {correlation_type} {strength} (r={corr_value:.3f})",
                statistical_significance=0.05,  # Assumindo significÃ¢ncia
                effect_size=abs(corr_value),
                data_points=len(df),
                visualization_data={
                    'type': 'correlation',
                    'variables': [var1, var2],
                    'correlation': corr_value
                },
                actionable_recommendation=self._generate_correlation_recommendation(var1, var2, corr_value),
                confidence_level='high' if abs(corr_value) > 0.7 else 'medium'
            )
            
            insights.append(insight)
        
        return insights
    
    def perform_user_clustering(self, df: pd.DataFrame) -> Tuple[List[UserClusterProfile], pd.DataFrame]:
        """ğŸ‘¥ Realiza clustering de usuÃ¡rios baseado em padrÃµes de performance"""
        self.logger.info("ğŸ‘¥ Realizando clustering de usuÃ¡rios...")
        
        # Agregar dados por usuÃ¡rio
        user_features = df.groupby('user_id').agg({
            'score': ['mean', 'std', 'min', 'max'],
            'improvement_rate': 'mean',
            'cycles_detected': 'mean',
            'phase_consistency': 'mean',
            'session_number': 'max',
            'preparation_score': 'mean',
            'contact_score': 'mean',
            'follow_through_score': 'mean'
        }).fillna(0)
        
        # Flatten column names
        user_features.columns = ['_'.join(col).strip() for col in user_features.columns]
        
        if len(user_features) < 3:
            return [], df
        
        # Normalizar features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(user_features)
        
        # Determinar nÃºmero Ã³timo de clusters
        n_clusters = min(4, max(2, len(user_features) // 2))
        
        # Aplicar K-means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(features_scaled)
        
        # Adicionar cluster labels ao DataFrame original
        user_clusters = pd.Series(cluster_labels, index=user_features.index)
        df_with_clusters = df.copy()
        df_with_clusters['cluster'] = df_with_clusters['user_id'].map(user_clusters)
        
        # Analisar caracterÃ­sticas dos clusters
        cluster_profiles = []
        
        for cluster_id in range(n_clusters):
            cluster_users = user_features[cluster_labels == cluster_id]
            cluster_data = df_with_clusters[df_with_clusters['cluster'] == cluster_id]
            
            # Calcular caracterÃ­sticas tÃ­picas
            characteristics = {}
            for col in user_features.columns:
                characteristics[col] = cluster_users[col].mean()
            
            # Determinar padrÃ£o de melhoria
            avg_improvement = characteristics.get('improvement_rate_mean', 0)
            if avg_improvement > 1.0:
                improvement_pattern = 'rapid_learner'
                cluster_name = f"Aprendizes RÃ¡pidos (Cluster {cluster_id})"
            elif avg_improvement > 0:
                improvement_pattern = 'steady_improver'
                cluster_name = f"Melhoria Constante (Cluster {cluster_id})"
            else:
                improvement_pattern = 'plateau'
                cluster_name = f"PlatÃ´ de Performance (Cluster {cluster_id})"
            
            # Gerar recomendaÃ§Ãµes baseadas no cluster
            recommended_approach = self._generate_cluster_recommendations(characteristics, improvement_pattern)
            
            profile = UserClusterProfile(
                cluster_id=cluster_id,
                cluster_name=cluster_name,
                typical_characteristics=characteristics,
                user_count=len(cluster_users),
                improvement_pattern=improvement_pattern,
                recommended_approach=recommended_approach,
                success_factors=self._identify_success_factors(cluster_data)
            )
            
            cluster_profiles.append(profile)
        
        return cluster_profiles, df_with_clusters
    
    def detect_performance_anomalies(self, df: pd.DataFrame) -> List[StatisticalInsight]:
        """ğŸš¨ Detecta anomalias e outliers na performance"""
        self.logger.info("ğŸš¨ Detectando anomalias de performance...")
        
        insights = []
        
        if df.empty or 'score' not in df.columns:
            return insights
        
        # Usar Isolation Forest para detecÃ§Ã£o de anomalias
        features = ['score', 'cycles_detected', 'phase_consistency']
        available_features = [f for f in features if f in df.columns]
        
        if len(available_features) < 2:
            return insights
        
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = iso_forest.fit_predict(df[available_features].fillna(0))
        
        anomalies = df[anomaly_labels == -1]
        
        if not anomalies.empty:
            # Analisar tipos de anomalias
            for idx, anomaly in anomalies.iterrows():
                anomaly_description = self._describe_anomaly(anomaly, df)
                
                insight = StatisticalInsight(
                    insight_type='anomaly',
                    title="Anomalia de Performance Detectada",
                    description=anomaly_description,
                    statistical_significance=0.1,  # Baseado na contaminaÃ§Ã£o do Isolation Forest
                    effect_size=abs(anomaly['score'] - df['score'].mean()) / df['score'].std(),
                    data_points=1,
                    visualization_data={
                        'type': 'anomaly',
                        'anomaly_data': anomaly.to_dict(),
                        'context_mean': df['score'].mean()
                    },
                    actionable_recommendation=self._generate_anomaly_recommendation(anomaly),
                    confidence_level='medium'
                )
                
                insights.append(insight)
        
        return insights[:3]  # Limitar a 3 anomalias mais relevantes
    
    def build_predictive_model(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ğŸ”® ConstrÃ³i modelo preditivo para performance futura"""
        self.logger.info("ğŸ”® Construindo modelo preditivo...")
        
        if df.empty or len(df) < 10:
            return {'success': False, 'reason': 'Dados insuficientes'}
        
        # Preparar features para prediÃ§Ã£o
        feature_cols = ['session_number', 'cycles_detected', 'preparation_score', 
                       'contact_score', 'follow_through_score', 'phase_consistency']
        
        available_features = [col for col in feature_cols if col in df.columns]
        
        if len(available_features) < 3:
            return {'success': False, 'reason': 'Features insuficientes'}
        
        # Preparar dados
        X = df[available_features].fillna(0)
        y = df['score']
        
        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        # Treinar Random Forest
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        # Avaliar modelo
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        # Feature importance
        feature_importance = dict(zip(available_features, model.feature_importances_))
        
        # PrediÃ§Ãµes para prÃ³ximas sessÃµes
        future_predictions = {}
        for user_id in df['user_id'].unique():
            user_data = df[df['user_id'] == user_id]
            if len(user_data) >= 3:
                last_session = user_data.iloc[-1]
                next_session_features = last_session[available_features].values.reshape(1, -1)
                next_session_features[0][0] += 1  # Incrementar session_number
                
                predicted_score = model.predict(next_session_features)[0]
                future_predictions[user_id] = predicted_score
        
        return {
            'success': True,
            'model_performance': {
                'r2_score': r2,
                'rmse': rmse,
                'feature_importance': feature_importance
            },
            'future_predictions': future_predictions,
            'model_confidence': 'high' if r2 > 0.7 else 'medium' if r2 > 0.4 else 'low'
        }
    
    def analyze_temporal_patterns(self, df: pd.DataFrame) -> List[StatisticalInsight]:
        """â° Analisa padrÃµes temporais e sazonalidades"""
        self.logger.info("â° Analisando padrÃµes temporais...")
        
        insights = []
        
        if df.empty or 'timestamp' not in df.columns:
            return insights
        
        df_temporal = df.copy()
        df_temporal['hour'] = df_temporal['timestamp'].dt.hour
        df_temporal['day_of_week'] = df_temporal['timestamp'].dt.dayofweek
        df_temporal['week_of_year'] = df_temporal['timestamp'].dt.isocalendar().week
        
        # Analisar performance por hora do dia
        if 'hour' in df_temporal.columns:
            hourly_performance = df_temporal.groupby('hour')['score'].agg(['mean', 'count']).reset_index()
            
            # Filtrar horas com dados suficientes
            significant_hours = hourly_performance[hourly_performance['count'] >= 3]
            
            if not significant_hours.empty:
                best_hour = significant_hours.loc[significant_hours['mean'].idxmax()]
                worst_hour = significant_hours.loc[significant_hours['mean'].idxmin()]
                
                if best_hour['mean'] - worst_hour['mean'] > 5:  # DiferenÃ§a significativa
                    insight = StatisticalInsight(
                        insight_type='trend',
                        title="PadrÃ£o de Performance por HorÃ¡rio",
                        description=f"Performance Ã© {best_hour['mean']:.1f}% melhor Ã s {best_hour['hour']:02d}:00 "
                                  f"comparado Ã s {worst_hour['hour']:02d}:00 ({worst_hour['mean']:.1f}%)",
                        statistical_significance=0.05,
                        effect_size=(best_hour['mean'] - worst_hour['mean']) / df['score'].std(),
                        data_points=len(significant_hours),
                        visualization_data={
                            'type': 'temporal',
                            'data': hourly_performance.to_dict('records')
                        },
                        actionable_recommendation=f"Agende treinos preferencialmente Ã s {best_hour['hour']:02d}:00 para melhor performance",
                        confidence_level='medium'
                    )
                    
                    insights.append(insight)
        
        # Analisar tendÃªncia de longo prazo
        if len(df) >= 10:
            # RegressÃ£o linear sobre tempo
            days_numeric = df_temporal['days_since_start'].values.reshape(-1, 1)
            scores = df_temporal['score'].values
            
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                df_temporal['days_since_start'], scores
            )
            
            if p_value < 0.05:  # TendÃªncia significativa
                trend_direction = "melhorando" if slope > 0 else "declinando"
                
                insight = StatisticalInsight(
                    insight_type='trend',
                    title=f"TendÃªncia de Longo Prazo: {trend_direction.title()}",
                    description=f"Performance {trend_direction} {abs(slope):.2f} pontos por dia "
                              f"(rÂ²={r_value**2:.3f}, p={p_value:.3f})",
                    statistical_significance=p_value,
                    effect_size=abs(r_value),
                    data_points=len(df),
                    visualization_data={
                        'type': 'trend',
                        'slope': slope,
                        'intercept': intercept,
                        'r_squared': r_value**2
                    },
                    actionable_recommendation=self._generate_trend_recommendation(slope, r_value**2),
                    confidence_level='high' if p_value < 0.01 else 'medium'
                )
                
                insights.append(insight)
        
        return insights
    
    def _generate_correlation_recommendation(self, var1: str, var2: str, corr_value: float) -> str:
        """ğŸ’¡ Gera recomendaÃ§Ã£o baseada em correlaÃ§Ã£o"""
        recommendations = {
            ('score', 'cycles_detected'): "Pratique movimentos mais longos para detectar mais ciclos e melhorar score",
            ('score', 'phase_consistency'): "Foque na consistÃªncia entre fases do movimento",
            ('preparation_score', 'contact_score'): "Trabalhe preparaÃ§Ã£o e contato em conjunto",
            ('improvement_rate', 'session_number'): "Mantenha frequÃªncia regular de treinos para melhoria contÃ­nua"
        }
        
        key = tuple(sorted([var1, var2]))
        return recommendations.get(key, f"Monitore relaÃ§Ã£o entre {var1} e {var2} para otimizar performance")
    
    def _generate_cluster_recommendations(self, characteristics: Dict, pattern: str) -> str:
        """ğŸ¯ Gera recomendaÃ§Ãµes baseadas no cluster"""
        if pattern == 'rapid_learner':
            return "Continue explorando tÃ©cnicas avanÃ§adas e mantenha desafios progressivos"
        elif pattern == 'steady_improver':
            return "Mantenha consistÃªncia no treino e foque em refinamentos graduais"
        else:
            return "Varie exercÃ­cios e busque feedback mais frequente para quebrar platÃ´"
    
    def _identify_success_factors(self, cluster_data: pd.DataFrame) -> List[str]:
        """ğŸ† Identifica fatores de sucesso do cluster"""
        factors = []
        
        if not cluster_data.empty:
            avg_score = cluster_data['score'].mean()
            
            if avg_score > 70:
                factors.append("High performance scores")
            
            if cluster_data['phase_consistency'].mean() > 0.8:
                factors.append("Consistent phase execution")
            
            if cluster_data['improvement_rate'].mean() > 0.5:
                factors.append("Steady improvement rate")
        
        return factors[:3]
    
    def _describe_anomaly(self, anomaly: pd.Series, df: pd.DataFrame) -> str:
        """ğŸ” Descreve uma anomalia detectada"""
        score = anomaly['score']
        mean_score = df['score'].mean()
        
        if score > mean_score + 2 * df['score'].std():
            return f"Performance excepcionalmente alta ({score:.1f}% vs mÃ©dia {mean_score:.1f}%)"
        elif score < mean_score - 2 * df['score'].std():
            return f"Performance excepcionalmente baixa ({score:.1f}% vs mÃ©dia {mean_score:.1f}%)"
        else:
            return f"PadrÃ£o atÃ­pico detectado em mÃºltiplas mÃ©tricas"
    
    def _generate_anomaly_recommendation(self, anomaly: pd.Series) -> str:
        """ğŸš¨ Gera recomendaÃ§Ã£o para anomalia"""
        return "Investigue condiÃ§Ãµes especÃ­ficas desta sessÃ£o para identificar fatores que causaram o resultado atÃ­pico"
    
    def _generate_trend_recommendation(self, slope: float, r_squared: float) -> str:
        """ğŸ“ˆ Gera recomendaÃ§Ã£o baseada em tendÃªncia"""
        if slope > 0 and r_squared > 0.5:
            return "Excelente progresso! Mantenha a rotina atual de treinamento"
        elif slope > 0:
            return "Progresso positivo detectado. Considere otimizar frequÃªncia de treinos"
        elif slope < 0:
            return "TendÃªncia de declÃ­nio detectada. Revise mÃ©todo de treino e considere descanso"
        else:
            return "Performance estÃ¡vel. Considere variar exercÃ­cios para evitar platÃ´"
    
    def generate_comprehensive_report(self, df: pd.DataFrame) -> str:
        """ğŸ“„ Gera relatÃ³rio estatÃ­stico completo"""
        self.logger.info("ğŸ“„ Gerando relatÃ³rio estatÃ­stico completo...")
        
        report_sections = []
        
        # Header
        report_sections.append("ğŸ“Š RELATÃ“RIO ESTATÃSTICO AVANÃ‡ADO - TABLETENNISANALYZER v2.0")
        report_sections.append("=" * 70)
        report_sections.append(f"Data de GeraÃ§Ã£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_sections.append(f"Dados Analisados: {len(df)} registros de {df['user_id'].nunique() if not df.empty else 0} usuÃ¡rios")
        report_sections.append("")
        
        if df.empty:
            report_sections.append("âš ï¸ Nenhum dado disponÃ­vel para anÃ¡lise")
            return "\n".join(report_sections)
        
        # EstatÃ­sticas descritivas
        report_sections.append("ğŸ“ˆ ESTATÃSTICAS DESCRITIVAS:")
        report_sections.append(f"   Score MÃ©dio: {df['score'].mean():.1f}% (Â±{df['score'].std():.1f})")
        report_sections.append(f"   Score MÃ­nimo: {df['score'].min():.1f}%")
        report_sections.append(f"   Score MÃ¡ximo: {df['score'].max():.1f}%")
        report_sections.append(f"   Ciclos MÃ©dios Detectados: {df['cycles_detected'].mean():.1f}")
        report_sections.append("")
        
        # AnÃ¡lise de correlaÃ§Ãµes
        correlations = self.analyze_correlation_patterns(df)
        if correlations:
            report_sections.append("ğŸ”— CORRELAÃ‡Ã•ES SIGNIFICATIVAS:")
            for corr in correlations[:3]:
                report_sections.append(f"   â€¢ {corr.title}: {corr.description}")
                report_sections.append(f"     RecomendaÃ§Ã£o: {corr.actionable_recommendation}")
            report_sections.append("")
        
        # Clustering de usuÃ¡rios
        clusters, df_clustered = self.perform_user_clustering(df)
        if clusters:
            report_sections.append("ğŸ‘¥ ANÃLISE DE CLUSTERS DE USUÃRIOS:")
            for cluster in clusters:
                report_sections.append(f"   â€¢ {cluster.cluster_name} ({cluster.user_count} usuÃ¡rios)")
                report_sections.append(f"     PadrÃ£o: {cluster.improvement_pattern}")
                report_sections.append(f"     Abordagem: {cluster.recommended_approach}")
            report_sections.append("")
        
        # Anomalias
        anomalies = self.detect_performance_anomalies(df)
        if anomalies:
            report_sections.append("ğŸš¨ ANOMALIAS DETECTADAS:")
            for anomaly in anomalies:
                report_sections.append(f"   â€¢ {anomaly.description}")
            report_sections.append("")
        
        # AnÃ¡lise temporal
        temporal_insights = self.analyze_temporal_patterns(df)
        if temporal_insights:
            report_sections.append("â° PADRÃ•ES TEMPORAIS:")
            for insight in temporal_insights:
                report_sections.append(f"   â€¢ {insight.description}")
                report_sections.append(f"     RecomendaÃ§Ã£o: {insight.actionable_recommendation}")
            report_sections.append("")
        
        # Modelo preditivo
        prediction_model = self.build_predictive_model(df)
        if prediction_model.get('success'):
            report_sections.append("ğŸ”® MODELO PREDITIVO:")
            performance = prediction_model['model_performance']
            report_sections.append(f"   PrecisÃ£o (RÂ²): {performance['r2_score']:.3f}")
            report_sections.append(f"   Erro MÃ©dio (RMSE): {performance['rmse']:.1f}")
            report_sections.append(f"   ConfianÃ§a: {prediction_model['model_confidence']}")
            
            if prediction_model['future_predictions']:
                report_sections.append("   PrediÃ§Ãµes PrÃ³xima SessÃ£o:")
                for user, pred_score in list(prediction_model['future_predictions'].items())[:3]:
                    report_sections.append(f"     {user}: {pred_score:.1f}%")
            report_sections.append("")
        
        # ConclusÃµes
        report_sections.append("âœ… CONCLUSÃ•ES E RECOMENDAÃ‡Ã•ES GERAIS:")
        
        avg_score = df['score'].mean()
        if avg_score > 70:
            report_sections.append("   â€¢ Performance geral excelente - manter estratÃ©gia atual")
        elif avg_score > 60:
            report_sections.append("   â€¢ Performance boa com espaÃ§o para melhoria especÃ­fica")
        else:
            report_sections.append("   â€¢ Foco em fundamentos e consistÃªncia recomendado")
        
        if df['improvement_rate'].mean() > 0:
            report_sections.append("   â€¢ TendÃªncia positiva de melhoria detectada")
        else:
            report_sections.append("   â€¢ Revisar estratÃ©gias de treinamento para quebrar estagnaÃ§Ã£o")
        
        report_sections.append("")
        report_sections.append("ğŸ“Š AnÃ¡lise gerada pelo Advanced Statistics Engine")
        
        return "\n".join(report_sections)

def main():
    """ğŸš€ DemonstraÃ§Ã£o do Advanced Statistics Engine"""
    print("ğŸ“Š ADVANCED STATISTICS ENGINE - DEMONSTRAÃ‡ÃƒO")
    print("=" * 55)
    
    engine = AdvancedStatisticsEngine()
    
    # Coletar dados
    print("\nğŸ“ˆ Coletando dados de usuÃ¡rios...")
    df = engine.collect_all_user_data()
    
    if df.empty:
        print("âš ï¸ Nenhum dado encontrado. Gerando dados de demonstraÃ§Ã£o...")
        demo_data = engine._generate_demo_data()
        df = pd.DataFrame(demo_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['days_since_start'] = (df['timestamp'] - df['timestamp'].min()).dt.days
        df['session_number'] = df.groupby('user_id').cumcount() + 1
        df['improvement_rate'] = df.groupby('user_id')['score'].transform(
            lambda x: x.diff().fillna(0)
        )
    
    print(f"âœ… Dados coletados: {len(df)} registros de {df['user_id'].nunique()} usuÃ¡rios")
    
    # Executar anÃ¡lises
    print("\nğŸ”— Analisando correlaÃ§Ãµes...")
    correlations = engine.analyze_correlation_patterns(df)
    print(f"âœ… {len(correlations)} correlaÃ§Ãµes significativas encontradas")
    
    print("\nğŸ‘¥ Realizando clustering de usuÃ¡rios...")
    clusters, df_clustered = engine.perform_user_clustering(df)
    print(f"âœ… {len(clusters)} clusters identificados")
    
    print("\nğŸš¨ Detectando anomalias...")
    anomalies = engine.detect_performance_anomalies(df)
    print(f"âœ… {len(anomalies)} anomalias detectadas")
    
    print("\nğŸ”® Construindo modelo preditivo...")
    prediction_model = engine.build_predictive_model(df)
    if prediction_model.get('success'):
        print(f"âœ… Modelo criado com RÂ² = {prediction_model['model_performance']['r2_score']:.3f}")
    
    print("\nâ° Analisando padrÃµes temporais...")
    temporal_insights = engine.analyze_temporal_patterns(df)
    print(f"âœ… {len(temporal_insights)} padrÃµes temporais identificados")
    
    # Gerar relatÃ³rio
    print("\nğŸ“„ Gerando relatÃ³rio completo...")
    report = engine.generate_comprehensive_report(df)
    
    # Salvar relatÃ³rio
    report_path = Path("statistical_analysis_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ RelatÃ³rio salvo em: {report_path}")
    
    # Mostrar resumo dos resultados
    print(f"\nğŸ“Š RESUMO DOS RESULTADOS:")
    print(f"   ğŸ”— CorrelaÃ§Ãµes: {len(correlations)}")
    print(f"   ğŸ‘¥ Clusters: {len(clusters)}")
    print(f"   ğŸš¨ Anomalias: {len(anomalies)}")
    print(f"   â° PadrÃµes temporais: {len(temporal_insights)}")
    print(f"   ğŸ”® Modelo preditivo: {'âœ… Sucesso' if prediction_model.get('success') else 'âŒ Falha'}")
    
    print(f"\nâœ… AnÃ¡lise estatÃ­stica concluÃ­da!")

if __name__ == "__main__":
    main()
