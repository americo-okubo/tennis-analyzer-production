#!/usr/bin/env python3
"""
üìä ADVANCED STATISTICS ENGINE
Sistema de an√°lise estat√≠stica avan√ßada para dados de t√™nis de mesa.
Correla√ß√µes, regress√µes, an√°lise preditiva e insights estat√≠sticos profundos.
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import logging
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import warnings

# Imports do sistema
from progression_analyzer import ProgressionAnalyzer
from fast_comparison_engine import FastComparisonEngine
from benchmark_optimizer import BenchmarkOptimizer

warnings.filterwarnings('ignore')

@dataclass
class StatisticalInsight:
    """Insight estat√≠stico individual"""
    insight_type: str  # 'correlation', 'trend', 'anomaly', 'prediction'
    title: str
    description: str
    statistical_significance: float  # p-value or confidence
    effect_size: float  # magnitude of effect
    data_points: int
    visualization_data: Optional[Dict[str, Any]]
    actionable_recommendation: str
    confidence_level: str  # 'high', 'medium', 'low'

@dataclass
class UserClusterProfile:
    """Perfil de cluster de usu√°rios"""
    cluster_id: int
    cluster_name: str
    typical_characteristics: Dict[str, float]
    user_count: int
    improvement_pattern: str
    recommended_approach: str
    success_factors: List[str]

class AdvancedStatisticsEngine:
    """
    üìä Motor de Estat√≠sticas Avan√ßadas
    An√°lise estat√≠stica profunda de dados de performance e progress√£o
    """
    
    def __init__(self, data_directory: str = "./"):
        self.data_dir = Path(data_directory)
        
        # Componentes do sistema
        self.progression_analyzer = ProgressionAnalyzer()
        self.fast_engine = FastComparisonEngine()
        
        # Cache de dados
        self._data_cache = {}
        self._analysis_cache = {}
        
        # Configura√ß√£o de visualiza√ß√£o
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        print("üìä Advanced Statistics Engine inicializado")
    
    def collect_all_user_data(self) -> pd.DataFrame:
        """üìà Coleta todos os dados de usu√°rios dispon√≠veis"""
        self.logger.info("üìà Coletando dados de todos os usu√°rios...")
        
        all_data = []
        
        # Buscar arquivos de progress√£o
        progression_files = list(self.data_dir.glob("progression_database/*_progression.json"))
        
        for prog_file in progression_files:
            try:
                with open(prog_file, 'r', encoding='utf-8') as f:
                    user_data = json.load(f)
                
                user_id = user_data['user_id']
                
                for entry in user_data.get('entries', []):
                    # Extrair dados estruturados
                    data_point = {
                        'user_id': user_id,
                        'timestamp': entry['timestamp'],
                        'score': entry['score'],
                        'confidence': entry['confidence'],
                        'cycles_detected': entry['cycles_detected'],
                        'movement': entry['movement'],
                        'professional_reference': entry['professional_reference'],
                        'preparation_score': entry.get('phase_scores', {}).get('preparation', 0),
                        'contact_score': entry.get('phase_scores', {}).get('contact', 0),
                        'follow_through_score': entry.get('phase_scores', {}).get('follow_through', 0),
                        'overall_quality_score': entry.get('phase_scores', {}).get('overall_quality', 0),
                    }
                    
                    # Adicionar m√©tricas t√©cnicas se dispon√≠veis
                    if 'technical_metrics' in entry:
                        tech_metrics = entry['technical_metrics']
                        data_point.update({
                            'phase_consistency': tech_metrics.get('phase_consistency', 0),
                            'cycle_ratio': tech_metrics.get('cycle_ratio', 0),
                            'confidence_numeric': tech_metrics.get('confidence_numeric', 0)
                        })
                    
                    all_data.append(data_point)
                    
            except Exception as e:
                self.logger.warning(f"Erro ao processar {prog_file}: {e}")
        
        if not all_data:
            # Gerar dados simulados para demonstra√ß√£o
            all_data = self._generate_demo_data()
        
        df = pd.DataFrame(all_data)
        
        # Processamento adicional
        if not df.empty:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['days_since_start'] = (df['timestamp'] - df['timestamp'].min()).dt.days
            df['session_number'] = df.groupby('user_id').cumcount() + 1
            
            # Calcular m√©tricas derivadas
            df['improvement_rate'] = df.groupby('user_id')['score'].transform(
                lambda x: x.diff().fillna(0)
            )
            
            df['cumulative_improvement'] = df.groupby('user_id')['improvement_rate'].cumsum()
            
        self.logger.info(f"üìä Coletados {len(df)} pontos de dados de {df['user_id'].nunique() if not df.empty else 0} usu√°rios")
        
        return df
    
    def _generate_demo_data(self) -> List[Dict]:
        """üé≤ Gera dados simulados para demonstra√ß√£o"""
        demo_data = []
        users = ['Americo', 'Baixinha', 'Gordo', 'Usuario1', 'Usuario2']
        professionals = ['Ma_Long', 'Fan_Zhendong', 'Zhang_Jike', 'Timo_Boll']
        movements = ['FD', 'BD', 'FP']
        
        base_date = datetime.now() - timedelta(days=90)
        
        for user in users:
            sessions = np.random.randint(5, 25)  # 5-25 sess√µes por usu√°rio
            base_score = np.random.uniform(45, 75)  # Score inicial
            improvement_rate = np.random.uniform(-0.2, 1.5)  # Taxa de melhoria
            
            for session in range(sessions):
                date = base_date + timedelta(days=session * np.random.randint(1, 5))
                
                # Score com tend√™ncia e ru√≠do
                score = base_score + (session * improvement_rate) + np.random.normal(0, 3)
                score = np.clip(score, 20, 95)  # Limitar score
                
                # Gerar scores por fase
                phase_variance = np.random.uniform(0.8, 1.2, 4)
                phases = {
                    'preparation': score * phase_variance[0],
                    'contact': score * phase_variance[1], 
                    'follow_through': score * phase_variance[2],
                    'overall_quality': score * phase_variance[3]
                }
                
                demo_data.append({
                    'user_id': user,
                    'timestamp': date.isoformat(),
                    'score': score,
                    'confidence': np.random.choice(['high', 'medium', 'low'], p=[0.5, 0.3, 0.2]),
                    'cycles_detected': np.random.randint(2, 8),
                    'movement': np.random.choice(movements),
                    'professional_reference': np.random.choice(professionals),
                    'preparation_score': phases['preparation'],
                    'contact_score': phases['contact'],
                    'follow_through_score': phases['follow_through'],
                    'overall_quality_score': phases['overall_quality'],
                    'phase_consistency': np.random.uniform(0.6, 0.9),
                    'cycle_ratio': np.random.uniform(0.3, 1.2),
                    'confidence_numeric': {'high': 1.0, 'medium': 0.6, 'low': 0.3}[
                        np.random.choice(['high', 'medium', 'low'], p=[0.5, 0.3, 0.2])
                    ]
                })
        
        return demo_data
    
    def analyze_correlation_patterns(self, df: pd.DataFrame) -> List[StatisticalInsight]:
        """üìà Analisa padr√µes de correla√ß√£o entre vari√°veis"""
        self.logger.info("üìà Analisando padr√µes de correla√ß√£o...")
        
        insights = []
        
        # Selecionar vari√°veis num√©ricas
        numeric_cols = ['score', 'cycles_detected', 'preparation_score', 'contact_score',
                       'follow_through_score', 'overall_quality_score', 'phase_consistency',
                       'cycle_ratio', 'confidence_numeric', 'session_number', 'improvement_rate']
        
        available_cols = [col for col in numeric_cols if col in df.columns]
        
        if len(available_cols) < 2:
            return insights
        
        # Calcular matriz de correla√ß√£o
        corr_matrix = df[available_cols].corr()
        
        # Encontrar correla√ß√µes significativas
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > 0.5:  # Correla√ß√£o moderada a forte
                    var1 = corr_matrix.columns[i]
                    var2 = corr_matrix.columns[j]
                    strong_correlations.append((var1, var2, corr_value))
        
        # Gerar insights para correla√ß√µes fortes
        for var1, var2, corr_value in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True)[:5]:
            correlation_type = "positiva" if corr_value > 0 else "negativa"
            strength = "forte" if abs(corr_value) > 0.7 else "moderada"
            
            insight = StatisticalInsight(
                insight_type='correlation',
                title=f"Correla√ß√£o {strength} {correlation_type}",
                description=f"{var1} e {var2} mostram correla√ß√£o {correlation_type} {strength} (r={corr_value:.3f})",
                statistical_significance=0.05,  # Assumindo signific√¢ncia
                effect_size=abs(corr_value),
                data_points=len(df),
                visualization_data={
                    'type': 'correlation',
                    'variables': [var1, var2],
                    'correlation': corr_value
                },
                actionable_recommendation=self._generate_correlation_recommendation(var1, var2, corr_value),
                confidence_level='high' if abs(corr_value) > 0.7 else 'medium'
            )
            
            insights.append(insight)
        
        return insights
    
    def perform_user_clustering(self, df: pd.DataFrame) -> Tuple[List[UserClusterProfile], pd.DataFrame]:
        """üë• Realiza clustering de usu√°rios baseado em padr√µes de performance"""
        self.logger.info("üë• Realizando clustering de usu√°rios...")
        
        # Agregar dados por usu√°rio
        user_features = df.groupby('user_id').agg({
            'score': ['mean', 'std', 'min', 'max'],
            'improvement_rate': 'mean',
            'cycles_detected': 'mean',
            'phase_consistency': 'mean',
            'session_number': 'max',
            'preparation_score': 'mean',
            'contact_score': 'mean',
            'follow_through_score': 'mean'
        }).fillna(0)
        
        # Flatten column names
        user_features.columns = ['_'.join(col).strip() for col in user_features.columns]
        
        if len(user_features) < 3:
            return [], df
        
        # Normalizar features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(user_features)
        
        # Determinar n√∫mero √≥timo de clusters
        n_clusters = min(4, max(2, len(user_features) // 2))
        
        # Aplicar K-means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(features_scaled)
        
        # Adicionar cluster labels ao DataFrame original
        user_clusters = pd.Series(cluster_labels, index=user_features.index)
        df_with_clusters = df.copy()
        df_with_clusters['cluster'] = df_with_clusters['user_id'].map(user_clusters)
        
        # Analisar caracter√≠sticas dos clusters
        cluster_profiles = []
        
        for cluster_id in range(n_clusters):
            cluster_users = user_features[cluster_labels == cluster_id]
            cluster_data = df_with_clusters[df_with_clusters['cluster'] == cluster_id]
            
            # Calcular caracter√≠sticas t√≠picas
            characteristics = {}
            for col in user_features.columns:
                characteristics[col] = cluster_users[col].mean()
            
            # Determinar padr√£o de melhoria
            avg_improvement = characteristics.get('improvement_rate_mean', 0)
            if avg_improvement > 1.0:
                improvement_pattern = 'rapid_learner'
                cluster_name = f"Aprendizes R√°pidos (Cluster {cluster_id})"
            elif avg_improvement > 0:
                improvement_pattern = 'steady_improver'
                cluster_name = f"Melhoria Constante (Cluster {cluster_id})"
            else:
                improvement_pattern = 'plateau'
                cluster_name = f"Plat√¥ de Performance (Cluster {cluster_id})"
            
            # Gerar recomenda√ß√µes baseadas no cluster
            recommended_approach = self._generate_cluster_recommendations(characteristics, improvement_pattern)
            
            profile = UserClusterProfile(
                cluster_id=cluster_id,
                cluster_name=cluster_name,
                typical_characteristics=characteristics,
                user_count=len(cluster_users),
                improvement_pattern=improvement_pattern,
                recommended_approach=recommended_approach,
                success_factors=self._identify_success_factors(cluster_data)
            )
            
            cluster_profiles.append(profile)
        
        return cluster_profiles, df_with_clusters
    
    def detect_performance_anomalies(self, df: pd.DataFrame) -> List[StatisticalInsight]:
        """üö® Detecta anomalias e outliers na performance"""
        self.logger.info("üö® Detectando anomalias de performance...")
        
        insights = []
        
        if df.empty or 'score' not in df.columns:
            return insights
        
        # Usar Isolation Forest para detec√ß√£o de anomalias
        features = ['score', 'cycles_detected', 'phase_consistency']
        available_features = [f for f in features if f in df.columns]
        
        if len(available_features) < 2:
            return insights
        
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_labels = iso_forest.fit_predict(df[available_features].fillna(0))
        
        anomalies = df[anomaly_labels == -1]
        
        if not anomalies.empty:
            # Analisar tipos de anomalias
            for idx, anomaly in anomalies.iterrows():
                anomaly_description = self._describe_anomaly(anomaly, df)
                
                insight = StatisticalInsight(
                    insight_type='anomaly',
                    title="Anomalia de Performance Detectada",
                    description=anomaly_description,
                    statistical_significance=0.1,  # Baseado na contamina√ß√£o do Isolation Forest
                    effect_size=abs(anomaly['score'] - df['score'].mean()) / df['score'].std(),
                    data_points=1,
                    visualization_data={
                        'type': 'anomaly',
                        'anomaly_data': anomaly.to_dict(),
                        'context_mean': df['score'].mean()
                    },
                    actionable_recommendation=self._generate_anomaly_recommendation(anomaly),
                    confidence_level='medium'
                )
                
                insights.append(insight)
        
        return insights[:3]  # Limitar a 3 anomalias mais relevantes
    
    def build_predictive_model(self, df: pd.DataFrame) -> Dict[str, Any]:
        """üîÆ Constr√≥i modelo preditivo para performance futura"""
        self.logger.info("üîÆ Construindo modelo preditivo...")
        
        if df.empty or len(df) < 10:
            return {'success': False, 'reason': 'Dados insuficientes'}
        
        # Preparar features para predi√ß√£o
        feature_cols = ['session_number', 'cycles_detected', 'preparation_score', 
                       'contact_score', 'follow_through_score', 'phase_consistency']
        
        available_features = [col for col in feature_cols if col in df.columns]
        
        if len(available_features) < 3:
            return {'success': False, 'reason': 'Features insuficientes'}
        
        # Preparar dados
        X = df[available_features].fillna(0)
        y = df['score']
        
        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        # Treinar Random Forest
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        # Avaliar modelo
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        # Feature importance
        feature_importance = dict(zip(available_features, model.feature_importances_))
        
        # Predi√ß√µes para pr√≥ximas sess√µes
        future_predictions = {}
        for user_id in df['user_id'].unique():
            user_data = df[df['user_id'] == user_id]
            if len(user_data) >= 3:
                last_session = user_data.iloc[-1]
                next_session_features = last_session[available_features].values.reshape(1, -1)
                next_session_features[0][0] += 1  # Incrementar session_number
                
                predicted_score = model.predict(next_session_features)[0]
                future_predictions[user_id] = predicted_score
        
        return {
            'success': True,
            'model_performance': {
                'r2_score': r2,
                'rmse': rmse,
                'feature_importance': feature_importance
            },
            'future_predictions': future_predictions,
            'model_confidence': 'high' if r2 > 0.7 else 'medium' if r2 > 0.4 else 'low'
        }
    
    def analyze_temporal_patterns(self, df: pd.DataFrame) -> List[StatisticalInsight]:
        """‚è∞ Analisa padr√µes temporais e sazonalidades"""
        self.logger.info("‚è∞ Analisando padr√µes temporais...")
        
        insights = []
        
        if df.empty or 'timestamp' not in df.columns:
            return insights
        
        df_temporal = df.copy()
        df_temporal['hour'] = df_temporal['timestamp'].dt.hour
        df_temporal['day_of_week'] = df_temporal['timestamp'].dt.dayofweek
        df_temporal['week_of_year'] = df_temporal['timestamp'].dt.isocalendar().week
        
        # Analisar performance por hora do dia
        if 'hour' in df_temporal.columns:
            hourly_performance = df_temporal.groupby('hour')['score'].agg(['mean', 'count']).reset_index()
            
            # Filtrar horas com dados suficientes
            significant_hours = hourly_performance[hourly_performance['count'] >= 3]
            
            if not significant_hours.empty:
                best_hour = significant_hours.loc[significant_hours['mean'].idxmax()]
                worst_hour = significant_hours.loc[significant_hours['mean'].idxmin()]
                
                if best_hour['mean'] - worst_hour['mean'] > 5:  # Diferen√ßa significativa
                    insight = StatisticalInsight(
                        insight_type='trend',
                        title="Padr√£o de Performance por Hor√°rio",
                        description=f"Performance √© {best_hour['mean']:.1f}% melhor √†s {best_hour['hour']:02d}:00 "
                                  f"comparado √†s {worst_hour['hour']:02d}:00 ({worst_hour['mean']:.1f}%)",
                        statistical_significance=0.05,
                        effect_size=(best_hour['mean'] - worst_hour['mean']) / df['score'].std(),
                        data_points=len(significant_hours),
                        visualization_data={
                            'type': 'temporal',
                            'data': hourly_performance.to_dict('records')
                        },
                        actionable_recommendation=f"Agende treinos preferencialmente √†s {best_hour['hour']:02d}:00 para melhor performance",
                        confidence_level='medium'
                    )
                    
                    insights.append(insight)
        
        # Analisar tend√™ncia de longo prazo
        if len(df) >= 10:
            # Regress√£o linear sobre tempo
            days_numeric = df_temporal['days_since_start'].values.reshape(-1, 1)
            scores = df_temporal['score'].values
            
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                df_temporal['days_since_start'], scores
            )
            
            if p_value < 0.05:  # Tend√™ncia significativa
                trend_direction = "melhorando" if slope > 0 else "declinando"
                
                insight = StatisticalInsight(
                    insight_type='trend',
                    title=f"Tend√™ncia de Longo Prazo: {trend_direction.title()}",
                    description=f"Performance {trend_direction} {abs(slope):.2f} pontos por dia "
                              f"(r¬≤={r_value**2:.3f}, p={p_value:.3f})",
                    statistical_significance=p_value,
                    effect_size=abs(r_value),
                    data_points=len(df),
                    visualization_data={
                        'type': 'trend',
                        'slope': slope,
                        'intercept': intercept,
                        'r_squared': r_value**2
                    },
                    actionable_recommendation=self._generate_trend_recommendation(slope, r_value**2),
                    confidence_level='high' if p_value < 0.01 else 'medium'
                )
                
                insights.append(insight)
        
        return insights
    
    def _generate_correlation_recommendation(self, var1: str, var2: str, corr_value: float) -> str:
        """üí° Gera recomenda√ß√£o baseada em correla√ß√£o"""
        recommendations = {
            ('score', 'cycles_detected'): "Pratique movimentos mais longos para detectar mais ciclos e melhorar score",
            ('score', 'phase_consistency'): "Foque na consist√™ncia entre fases do movimento",
            ('preparation_score', 'contact_score'): "Trabalhe prepara√ß√£o e contato em conjunto",
            ('improvement_rate', 'session_number'): "Mantenha frequ√™ncia regular de treinos para melhoria cont√≠nua"
        }
        
        key = tuple(sorted([var1, var2]))
        return recommendations.get(key, f"Monitore rela√ß√£o entre {var1} e {var2} para otimizar performance")
    
    def _generate_cluster_recommendations(self, characteristics: Dict, pattern: str) -> str:
        """üéØ Gera recomenda√ß√µes baseadas no cluster"""
        if pattern == 'rapid_learner':
            return "Continue explorando t√©cnicas avan√ßadas e mantenha desafios progressivos"
        elif pattern == 'steady_improver':
            return "Mantenha consist√™ncia no treino e foque em refinamentos graduais"
        else:
            return "Varie exerc√≠cios e busque feedback mais frequente para quebrar plat√¥"
    
    def _identify_success_factors(self, cluster_data: pd.DataFrame) -> List[str]:
        """üèÜ Identifica fatores de sucesso do cluster"""
        factors = []
        
        if not cluster_data.empty:
            avg_score = cluster_data['score'].mean()
            
            if avg_score > 70:
                factors.append("High performance scores")
            
            if cluster_data['phase_consistency'].mean() > 0.8:
                factors.append("Consistent phase execution")
            
            if cluster_data['improvement_rate'].mean() > 0.5:
                factors.append("Steady improvement rate")
        
        return factors[:3]
    
    def _describe_anomaly(self, anomaly: pd.Series, df: pd.DataFrame) -> str:
        """üîç Descreve uma anomalia detectada"""
        score = anomaly['score']
        mean_score = df['score'].mean()
        
        if score > mean_score + 2 * df['score'].std():
            return f"Performance excepcionalmente alta ({score:.1f}% vs m√©dia {mean_score:.1f}%)"
        elif score < mean_score - 2 * df['score'].std():
            return f"Performance excepcionalmente baixa ({score:.1f}% vs m√©dia {mean_score:.1f}%)"
        else:
            return f"Padr√£o at√≠pico detectado em m√∫ltiplas m√©tricas"
    
    def _generate_anomaly_recommendation(self, anomaly: pd.Series) -> str:
        """üö® Gera recomenda√ß√£o para anomalia"""
        return "Investigue condi√ß√µes espec√≠ficas desta sess√£o para identificar fatores que causaram o resultado at√≠pico"
    
    def _generate_trend_recommendation(self, slope: float, r_squared: float) -> str:
        """üìà Gera recomenda√ß√£o baseada em tend√™ncia"""
        if slope > 0 and r_squared > 0.5:
            return "Excelente progresso! Mantenha a rotina atual de treinamento"
        elif slope > 0:
            return "Progresso positivo detectado. Considere otimizar frequ√™ncia de treinos"
        elif slope < 0:
            return "Tend√™ncia de decl√≠nio detectada. Revise m√©todo de treino e considere descanso"
        else:
            return "Performance est√°vel. Considere variar exerc√≠cios para evitar plat√¥"
    
    def generate_comprehensive_report(self, df: pd.DataFrame) -> str:
        """üìÑ Gera relat√≥rio estat√≠stico completo"""
        self.logger.info("üìÑ Gerando relat√≥rio estat√≠stico completo...")
        
        report_sections = []
        
        # Header
        report_sections.append("üìä RELAT√ìRIO ESTAT√çSTICO AVAN√áADO - TABLETENNISANALYZER v2.0")
        report_sections.append("=" * 70)
        report_sections.append(f"Data de Gera√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_sections.append(f"Dados Analisados: {len(df)} registros de {df['user_id'].nunique() if not df.empty else 0} usu√°rios")
        report_sections.append("")
        
        if df.empty:
            report_sections.append("‚ö†Ô∏è Nenhum dado dispon√≠vel para an√°lise")
            return "\n".join(report_sections)
        
        # Estat√≠sticas descritivas
        report_sections.append("üìà ESTAT√çSTICAS DESCRITIVAS:")
        report_sections.append(f"   Score M√©dio: {df['score'].mean():.1f}% (¬±{df['score'].std():.1f})")
        report_sections.append(f"   Score M√≠nimo: {df['score'].min():.1f}%")
        report_sections.append(f"   Score M√°ximo: {df['score'].max():.1f}%")
        report_sections.append(f"   Ciclos M√©dios Detectados: {df['cycles_detected'].mean():.1f}")
        report_sections.append("")
        
        # An√°lise de correla√ß√µes
        correlations = self.analyze_correlation_patterns(df)
        if correlations:
            report_sections.append("üîó CORRELA√á√ïES SIGNIFICATIVAS:")
            for corr in correlations[:3]:
                report_sections.append(f"   ‚Ä¢ {corr.title}: {corr.description}")
                report_sections.append(f"     Recomenda√ß√£o: {corr.actionable_recommendation}")
            report_sections.append("")
        
        # Clustering de usu√°rios
        clusters, df_clustered = self.perform_user_clustering(df)
        if clusters:
            report_sections.append("üë• AN√ÅLISE DE CLUSTERS DE USU√ÅRIOS:")
            for cluster in clusters:
                report_sections.append(f"   ‚Ä¢ {cluster.cluster_name} ({cluster.user_count} usu√°rios)")
                report_sections.append(f"     Padr√£o: {cluster.improvement_pattern}")
                report_sections.append(f"     Abordagem: {cluster.recommended_approach}")
            report_sections.append("")
        
        # Anomalias
        anomalies = self.detect_performance_anomalies(df)
        if anomalies:
            report_sections.append("üö® ANOMALIAS DETECTADAS:")
            for anomaly in anomalies:
                report_sections.append(f"   ‚Ä¢ {anomaly.description}")
            report_sections.append("")
        
        # An√°lise temporal
        temporal_insights = self.analyze_temporal_patterns(df)
        if temporal_insights:
            report_sections.append("‚è∞ PADR√ïES TEMPORAIS:")
            for insight in temporal_insights:
                report_sections.append(f"   ‚Ä¢ {insight.description}")
                report_sections.append(f"     Recomenda√ß√£o: {insight.actionable_recommendation}")
            report_sections.append("")
        
        # Modelo preditivo
        prediction_model = self.build_predictive_model(df)
        if prediction_model.get('success'):
            report_sections.append("üîÆ MODELO PREDITIVO:")
            performance = prediction_model['model_performance']
            report_sections.append(f"   Precis√£o (R¬≤): {performance['r2_score']:.3f}")
            report_sections.append(f"   Erro M√©dio (RMSE): {performance['rmse']:.1f}")
            report_sections.append(f"   Confian√ßa: {prediction_model['model_confidence']}")
            
            if prediction_model['future_predictions']:
                report_sections.append("   Predi√ß√µes Pr√≥xima Sess√£o:")
                for user, pred_score in list(prediction_model['future_predictions'].items())[:3]:
                    report_sections.append(f"     {user}: {pred_score:.1f}%")
            report_sections.append("")
        
        # Conclus√µes
        report_sections.append("‚úÖ CONCLUS√ïES E RECOMENDA√á√ïES GERAIS:")
        
        avg_score = df['score'].mean()
        if avg_score > 70:
            report_sections.append("   ‚Ä¢ Performance geral excelente - manter estrat√©gia atual")
        elif avg_score > 60:
            report_sections.append("   ‚Ä¢ Performance boa com espa√ßo para melhoria espec√≠fica")
        else:
            report_sections.append("   ‚Ä¢ Foco em fundamentos e consist√™ncia recomendado")
        
        if df['improvement_rate'].mean() > 0:
            report_sections.append("   ‚Ä¢ Tend√™ncia positiva de melhoria detectada")
        else:
            report_sections.append("   ‚Ä¢ Revisar estrat√©gias de treinamento para quebrar estagna√ß√£o")
        
        report_sections.append("")
        report_sections.append("üìä An√°lise gerada pelo Advanced Statistics Engine")
        
        return "\n".join(report_sections)

def main():
    """üöÄ Demonstra√ß√£o do Advanced Statistics Engine"""
    print("üìä ADVANCED STATISTICS ENGINE - DEMONSTRA√á√ÉO")
    print("=" * 55)
    
    engine = AdvancedStatisticsEngine()
    
    # Coletar dados
    print("\nüìà Coletando dados de usu√°rios...")
    df = engine.collect_all_user_data()
    
    if df.empty:
        print("‚ö†Ô∏è Nenhum dado encontrado. Gerando dados de demonstra√ß√£o...")
        demo_data = engine._generate_demo_data()
        df = pd.DataFrame(demo_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['days_since_start'] = (df['timestamp'] - df['timestamp'].min()).dt.days
        df['session_number'] = df.groupby('user_id').cumcount() + 1
        df['improvement_rate'] = df.groupby('user_id')['score'].transform(
            lambda x: x.diff().fillna(0)
        )
    
    print(f"‚úÖ Dados coletados: {len(df)} registros de {df['user_id'].nunique()} usu√°rios")
    
    # Executar an√°lises
    print("\nüîó Analisando correla√ß√µes...")
    correlations = engine.analyze_correlation_patterns(df)
    print(f"‚úÖ {len(correlations)} correla√ß√µes significativas encontradas")
    
    print("\nüë• Realizando clustering de usu√°rios...")
    clusters, df_clustered = engine.perform_user_clustering(df)
    print(f"‚úÖ {len(clusters)} clusters identificados")
    
    print("\nüö® Detectando anomalias...")
    anomalies = engine.detect_performance_anomalies(df)
    print(f"‚úÖ {len(anomalies)} anomalias detectadas")
    
    print("\nüîÆ Construindo modelo preditivo...")
    prediction_model = engine.build_predictive_model(df)
    if prediction_model.get('success'):
        print(f"‚úÖ Modelo criado com R¬≤ = {prediction_model['model_performance']['r2_score']:.3f}")
    
    print("\n‚è∞ Analisando padr√µes temporais...")
    temporal_insights = engine.analyze_temporal_patterns(df)
    print(f"‚úÖ {len(temporal_insights)} padr√µes temporais identificados")
    
    # Gerar relat√≥rio
    print("\nüìÑ Gerando relat√≥rio completo...")
    report = engine.generate_comprehensive_report(df)
    
    # Salvar relat√≥rio
    report_path = Path("statistical_analysis_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"üìÑ Relat√≥rio salvo em: {report_path}")
    
    # Mostrar resumo dos resultados
    print(f"\nüìä RESUMO DOS RESULTADOS:")
    print(f"   üîó Correla√ß√µes: {len(correlations)}")
    print(f"   üë• Clusters: {len(clusters)}")
    print(f"   üö® Anomalias: {len(anomalies)}")
    print(f"   ‚è∞ Padr√µes temporais: {len(temporal_insights)}")
    print(f"   üîÆ Modelo preditivo: {'‚úÖ Sucesso' if prediction_model.get('success') else '‚ùå Falha'}")
    
    print(f"\n‚úÖ An√°lise estat√≠stica conclu√≠da!")

if __name__ == "__main__":
    main()
